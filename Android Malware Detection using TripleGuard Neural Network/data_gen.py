from sklearn.model_selection import train_test_split
from load_save import save, load
import numpy as np
import pandas as pd
from scipy.stats import boxcox
from nbeats import NBeatsNet  # Assuming you have a modified N-BEATS implementation
from sklearn.preprocessing import PowerTransformer, MinMaxScaler, LabelEncoder

def datagen():
    # Load the dataset
    dataset = pd.read_csv("dataset/Android_Malware.csv")
    # Select a subset of the dataset (you may adjust it based on your requirements)
    dataset = dataset[:]
    # Columns to drop
    drop_columns = ['Unnamed: 0', 'Flow ID', ' Source IP', ' Source Port',' Destination IP', ' Destination Port', ' Protocol',
                    ' Timestamp',' min_seg_size_forward',' Fwd Header Length.1',' Fwd Header Length',' Bwd Header Length']

    # Drop unnecessary columns
    dataset = dataset.drop(columns=drop_columns)

    # Display the modified dataset
    print(dataset)
    # Initialize LabelEncoder for categorical columns
    label_encoder = LabelEncoder()

    # Transform the 'Label' column to numeric values
    dataset['Label'] = label_encoder.fit_transform(dataset['Label'])

    # Drop rows with missing values and fill NaN values with 0
    dataset.dropna(inplace=True)
    dataset = dataset.fillna(0)
    # Separate labels and features
    labels = np.array(dataset['Label'])
    features_data = dataset.drop('Label', axis=1)


    # Normalize the features using MinMaxScaler
    scaler = MinMaxScaler()
    features_data_normalized = scaler.fit_transform(features_data)
    # Assuming 'numeric_object_columns' contains the names of your object columns with numeric values
    numeric_object_columns = [' CWE Flag Count', ' Down/Up Ratio', 'Fwd Avg Bytes/Bulk']
    # Convert object columns to unsigned floats
    for col in numeric_object_columns:
        dataset[col] = pd.to_numeric(dataset[col], errors='coerce').astype('float')

     #Preprocessing

    # 1)Improved Box-Cox transformation for numerical columns
    #statistical tool that transforms non-normal data into a normal distribution.
    numerical_cols = features_data.select_dtypes(include=['float64', 'int64']).columns
    for col in numerical_cols:
        if features_data[col].min() <= 0 and features_data[col].nunique() > 1:
            # Check and correct non-positive values
            features_data[col] = features_data[col] - features_data[col].min() + 1e-5

            # Apply Box-Cox transformation using scipy's boxcox
            features_data[col], _ = boxcox(features_data[col] + 1)  # Adding 1 to handle zero values

    # Check and remove constant columns
    constant_columns = features_data.columns[features_data.nunique() == 1]
    features_data = features_data.drop(columns=constant_columns)

    # Additional check and correction for mixed data types
    for col in features_data.columns:
        features_data[col] = pd.to_numeric(features_data[col], errors='coerce')
    print('features', dataset.shape)

    # 2) Interquartile Range (IQR)
    # Identify and handle outliers using Interquartile Range (IQR)
    Q1 = features_data.quantile(0.25)
    Q3 = features_data.quantile(0.75)
    IQR = Q3 - Q1
    # Remove outliers
    features_no_outliers = features_data[
        ~((features_data < (Q1 - 1.5 * IQR)) | (features_data > (Q3 + 1.5 * IQR))).any(axis=1)]


    # Feature Extraction
    #1. to find Mean, Median, Variance, Standard deviation, mode
    # Calculate the mean along the rows (axis=1)
    mean = features_data.mean(axis=1)
    # Calculate the median along the rows (axis=1)
    median = np.median(features_data, axis=1)
    # Calculate the standard deviation along the rows (axis=1) for numeric columns
    std = features_data.std(axis=1)
    # Calculate the variance along the rows (axis=1)
    var = features_data.var(axis=1)
    # Calculate the mode along the rows (axis=1)
    mode =features_data.mode(axis=1)

    # Extract the mode(s) as a pandas Series
    mode = mode.iloc[:, 0]
    features_data = pd.DataFrame(features_data)
    skew = features_data.skew(axis=1)
    kurt = features_data.kurt(axis=1)
    # Create a list containing the Series for mean, median, mode, variance, and standard deviation
    feat1 = [mean, median, var, std, skew, kurt, mode]

    # Create a DataFrame 'df_stat' with the statistical measures as columns and rows as the original DataFrame's rows
    feat1 = pd.DataFrame(feat1, index=['mean', 'median', 'var', 'std', 'skew', 'kurt', 'mode']).T
    feat1.fillna(0, inplace=True)
    print(feat1)

    ## 2. Modified N-BEATS
    # Split dataset into train and test sets features_data, labels = datagen()
    # extracted_features = feature_extraction(features)
    input_shape =features_data.shape[1]  # Number of columns in the features DataFrame

    # Initialize the NBeatsNet model with the input shape
    nbeats_model = NBeatsNet(input_shape=input_shape)  # Initialize modified N-BEATS model
    # nbeats_model.fit(features)
    # Extract basis functions or features from the model
    extracted_features = nbeats_model.predict(features_data)  # Use pred
    feat2 = pd.DataFrame(extracted_features)

    # 3. Pearson Correlation coefficient
    # Calculate the Pearson correlation coefficients for each feature with respect to each row
    feat3 = dataset.iloc[:, :].corrwith(dataset.iloc[0, :].astype(float), axis=1)
    # Display the correlation values for each row
    print("Pearson Correlation Coefficients for Each Row:")
    print(feat3)

    # combined 3 dataset feat1,feat2 and feat3
    # Combine the datasets vertically (concatenate along the rows)
    feat = pd.concat([feat1, feat2, feat3, features_data], axis=1)
    feat.reset_index(drop=True, inplace=True)

    # Split the dataset into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(features_data, labels, test_size=0.2, random_state=42)
    save('X_train', X_train)
    save('X_test', X_test)
    save('y_train', y_train)
    save('y_test', y_test)

# Run the data generation function
#datagen()
